# -*- coding: utf-8 -*-
"""
æ›´æ–°åçš„ webui.py - ä½¿ç”¨ WebSocket ä¸Šä¼ åˆ°S3
å°†æ­¤æ–‡ä»¶æ›¿æ¢è¿œç¨‹æœåŠ¡å™¨ä¸Šçš„åŸwebui.py

WebSocket ä¼˜åŠ¿ï¼š
1. å®æ—¶åŒå‘é€šä¿¡
2. è¿æ¥ä¿æŒï¼Œæ— éœ€æ¯æ¬¡å»ºç«‹æ–°è¿æ¥
3. æ”¯æŒå¤§æ–‡ä»¶åˆ†å—ä¼ è¾“
4. å®æ—¶è¿›åº¦åé¦ˆ
"""

import json
import os
import base64
import asyncio
import threading
from concurrent.futures import ThreadPoolExecutor

# WebSocket ç›¸å…³
try:
    import websocket
    WEBSOCKET_AVAILABLE = True
except ImportError:
    WEBSOCKET_AVAILABLE = False
    print("âš ï¸ websocket-client not installed. Run: pip install websocket-client")

import requests  # å¤‡ç”¨ HTTP æ–¹å¼

# ===== WebSocket ä¸Šä¼ é…ç½® =====
# æ›¿æ¢ä¸ºæ‚¨çš„å®é™…å€¼
WEBSOCKET_URL = "wss://your-website.com/ws/upload"  # WebSocket ç«¯ç‚¹
RELAY_API_URL = "https://your-website.com/api/relay-to-s3"  # HTTP å¤‡ç”¨ç«¯ç‚¹
API_KEY = "YOUR_API_KEY_HERE"  # åœ¨æ‚¨çš„ç½‘ç«™ä¸ªäººèµ„æ–™é¡µé¢è·å–

# åˆ†å—å¤§å° (64KB)
CHUNK_SIZE = 64 * 1024

# çº¿ç¨‹æ± æ‰§è¡Œå™¨ï¼Œç”¨äºå¼‚æ­¥ä¸Šä¼ 
executor = ThreadPoolExecutor(max_workers=2)


class WebSocketUploader:
    """WebSocket æ–‡ä»¶ä¸Šä¼ å™¨"""
    
    def __init__(self, ws_url, api_key):
        self.ws_url = ws_url
        self.api_key = api_key
        self.ws = None
        self.connected = False
        self.upload_result = None
        self.upload_complete = threading.Event()
        
    def connect(self):
        """å»ºç«‹ WebSocket è¿æ¥"""
        if self.connected and self.ws:
            return True
            
        try:
            # åˆ›å»º WebSocket è¿æ¥ï¼Œå¸¦è®¤è¯å¤´
            self.ws = websocket.create_connection(
                self.ws_url,
                header=[f"Authorization: Bearer {self.api_key}"],
                timeout=30
            )
            self.connected = True
            print(f"âœ… WebSocket connected to {self.ws_url}")
            return True
        except Exception as e:
            print(f"âŒ WebSocket connection failed: {e}")
            self.connected = False
            return False
    
    def disconnect(self):
        """å…³é—­ WebSocket è¿æ¥"""
        if self.ws:
            try:
                self.ws.close()
            except:
                pass
            self.ws = None
            self.connected = False
    
    def upload_file(self, file_path, object_name=None):
        """
        é€šè¿‡ WebSocket ä¸Šä¼ æ–‡ä»¶
        
        Args:
            file_path: æœ¬åœ°æ–‡ä»¶è·¯å¾„
            object_name: åœ¨S3ä¸­çš„æ–‡ä»¶å
            
        Returns:
            str: S3å…¬å…±URLï¼Œå¦‚æœå¤±è´¥åˆ™è¿”å›None
        """
        if object_name is None:
            object_name = os.path.basename(file_path)
        
        # ç¡®ä¿è¿æ¥
        if not self.connect():
            print("âš ï¸ WebSocket unavailable, falling back to HTTP")
            return None
        
        try:
            file_size = os.path.getsize(file_path)
            print(f"ğŸ“¤ Uploading {object_name} ({file_size} bytes) via WebSocket...")
            
            # å‘é€å¼€å§‹ä¸Šä¼ æ¶ˆæ¯
            start_msg = {
                "action": "start_upload",
                "filename": object_name,
                "file_size": file_size,
                "content_type": self._get_content_type(file_path)
            }
            self.ws.send(json.dumps(start_msg))
            
            # ç­‰å¾…æœåŠ¡å™¨ç¡®è®¤
            response = json.loads(self.ws.recv())
            if response.get("status") != "ready":
                print(f"âŒ Server not ready: {response.get('error', 'Unknown error')}")
                return None
            
            upload_id = response.get("upload_id")
            print(f"ğŸ“‹ Upload ID: {upload_id}")
            
            # åˆ†å—è¯»å–å¹¶å‘é€æ–‡ä»¶
            chunk_index = 0
            with open(file_path, 'rb') as f:
                while True:
                    chunk = f.read(CHUNK_SIZE)
                    if not chunk:
                        break
                    
                    # å°†äºŒè¿›åˆ¶æ•°æ®ç¼–ç ä¸º base64
                    chunk_b64 = base64.b64encode(chunk).decode('utf-8')
                    
                    chunk_msg = {
                        "action": "upload_chunk",
                        "upload_id": upload_id,
                        "chunk_index": chunk_index,
                        "data": chunk_b64,
                        "is_last": len(chunk) < CHUNK_SIZE
                    }
                    self.ws.send(json.dumps(chunk_msg))
                    
                    # ç­‰å¾…ç¡®è®¤
                    ack = json.loads(self.ws.recv())
                    if ack.get("status") != "ok":
                        print(f"âŒ Chunk {chunk_index} failed: {ack.get('error')}")
                        return None
                    
                    chunk_index += 1
                    progress = min(100, int((chunk_index * CHUNK_SIZE / file_size) * 100))
                    print(f"   Progress: {progress}%", end='\r')
            
            print()  # æ¢è¡Œ
            
            # å‘é€å®Œæˆæ¶ˆæ¯
            complete_msg = {
                "action": "complete_upload",
                "upload_id": upload_id
            }
            self.ws.send(json.dumps(complete_msg))
            
            # ç­‰å¾…æœ€ç»ˆç»“æœ
            result = json.loads(self.ws.recv())
            if result.get("status") == "success":
                public_url = result.get("public_url")
                print(f"âœ… Upload successful: {public_url}")
                return public_url
            else:
                print(f"âŒ Upload failed: {result.get('error', 'Unknown error')}")
                return None
                
        except Exception as e:
            print(f"âŒ WebSocket upload error: {e}")
            self.disconnect()
            return None
    
    def _get_content_type(self, file_path):
        """æ ¹æ®æ–‡ä»¶æ‰©å±•åè·å– MIME ç±»å‹"""
        ext = os.path.splitext(file_path)[1].lower()
        content_types = {
            '.wav': 'audio/wav',
            '.mp3': 'audio/mpeg',
            '.ogg': 'audio/ogg',
            '.flac': 'audio/flac',
            '.png': 'image/png',
            '.jpg': 'image/jpeg',
            '.jpeg': 'image/jpeg',
            '.gif': 'image/gif',
            '.mp4': 'video/mp4',
            '.webm': 'video/webm',
            '.glb': 'model/gltf-binary',
        }
        return content_types.get(ext, 'application/octet-stream')


# å…¨å±€ WebSocket ä¸Šä¼ å™¨å®ä¾‹
ws_uploader = None

def get_ws_uploader():
    """è·å–æˆ–åˆ›å»º WebSocket ä¸Šä¼ å™¨"""
    global ws_uploader
    if ws_uploader is None:
        ws_uploader = WebSocketUploader(WEBSOCKET_URL, API_KEY)
    return ws_uploader


def upload_to_relay_server_http(file_path, object_name=None):
    """
    HTTP å¤‡ç”¨ä¸Šä¼ æ–¹å¼
    å½“ WebSocket ä¸å¯ç”¨æ—¶ä½¿ç”¨
    """
    if object_name is None:
        object_name = os.path.basename(file_path)
    
    try:
        with open(file_path, 'rb') as f:
            files = {'file': (object_name, f, 'audio/wav')}
            headers = {'Authorization': f'Bearer {API_KEY}'}
            
            print(f"ğŸ“¤ Uploading {object_name} via HTTP (fallback)...")
            response = requests.post(
                RELAY_API_URL,
                files=files,
                headers=headers,
                timeout=300
            )
        
        if response.status_code == 200:
            data = response.json()
            if data.get('success'):
                public_url = data.get('public_url')
                print(f"âœ… Uploaded to S3: {public_url}")
                return public_url
            else:
                print(f"âŒ Upload failed: {data.get('error', 'Unknown error')}")
                return None
        else:
            print(f"âŒ HTTP Error {response.status_code}: {response.text}")
            return None
            
    except Exception as e:
        print(f"âŒ HTTP Upload Error: {e}")
        return None


def upload_to_relay_server(file_path, object_name=None):
    """
    ä¸Šä¼ æ–‡ä»¶åˆ°ä¸­è½¬æœåŠ¡å™¨ - ä¼˜å…ˆä½¿ç”¨ WebSocket
    
    Args:
        file_path: æœ¬åœ°æ–‡ä»¶è·¯å¾„
        object_name: åœ¨S3ä¸­çš„æ–‡ä»¶å
    
    Returns:
        str: S3å…¬å…±URLï¼Œå¦‚æœå¤±è´¥åˆ™è¿”å›None
    """
    if object_name is None:
        object_name = os.path.basename(file_path)
    
    # ä¼˜å…ˆå°è¯• WebSocket
    if WEBSOCKET_AVAILABLE:
        uploader = get_ws_uploader()
        result = uploader.upload_file(file_path, object_name)
        if result:
            return result
        print("âš ï¸ WebSocket upload failed, trying HTTP fallback...")
    
    # HTTP å¤‡ç”¨
    return upload_to_relay_server_http(file_path, object_name)


def upload_async(file_path, object_name=None, callback=None):
    """
    å¼‚æ­¥ä¸Šä¼ æ–‡ä»¶ï¼ˆåœ¨åå°çº¿ç¨‹ä¸­æ‰§è¡Œï¼‰
    
    Args:
        file_path: æœ¬åœ°æ–‡ä»¶è·¯å¾„
        object_name: åœ¨S3ä¸­çš„æ–‡ä»¶å
        callback: ä¸Šä¼ å®Œæˆåçš„å›è°ƒå‡½æ•°ï¼Œæ¥æ”¶ (success, url_or_error) å‚æ•°
    """
    def _upload():
        try:
            url = upload_to_relay_server(file_path, object_name)
            if callback:
                callback(url is not None, url)
        except Exception as e:
            if callback:
                callback(False, str(e))
    
    executor.submit(_upload)


# ===== åŸ webui.py ä»£ç å¼€å§‹ =====

os.environ['HF_HUB_CACHE'] = '/gemini/code/checkpoints/hf_cache'
os.environ['TRANSFORMERS_CACHE'] = '/gemini/code/checkpoints/hf_cache'
os.environ['HF_HOME'] = '/gemini/code/checkpoints/hf_cache'
os.environ['WETEXT_CACHE'] = '/gemini/code/checkpoints/wetext_cache'
import sys
import threading
import time

import warnings

warnings.filterwarnings("ignore", category=FutureWarning)
warnings.filterwarnings("ignore", category=UserWarning)

import pandas as pd

current_dir = os.path.dirname(os.path.abspath(__file__))
sys.path.append(current_dir)
sys.path.append(os.path.join(current_dir, "indextts"))


print("""================ IndexTTS WebUI (WebSocket Edition) =================""")
import argparse
parser = argparse.ArgumentParser(
    description="IndexTTS WebUI",
    formatter_class=argparse.ArgumentDefaultsHelpFormatter,
)
parser.add_argument("--verbose", action="store_true", default=False, help="Enable verbose mode")
parser.add_argument("--port", type=int, default=7860, help="Port to run the web UI on")
parser.add_argument("--host", type=str, default="0.0.0.0", help="Host to run the web UI on")
parser.add_argument("--model_dir", type=str, default="/gemini/pretrain/IndexTTS-2", help="Model checkpoints directory")
parser.add_argument("--fp16", action="store_true", default=False, help="Use FP16 for inference if available")
parser.add_argument("--deepspeed", action="store_true", default=False, help="Use DeepSpeed to accelerate if available")
parser.add_argument("--cuda_kernel", action="store_true", default=False, help="Use CUDA kernel for inference if available")
parser.add_argument("--gui_seg_tokens", type=int, default=120, help="GUI: Max tokens per generation segment")
cmd_args = parser.parse_args()

import gradio as gr
from indextts.infer_v2 import IndexTTS2
from tools.i18n.i18n import I18nAuto

i18n = I18nAuto(language="Auto")
MODE = 'local'
tts = IndexTTS2(model_dir=cmd_args.model_dir,
                cfg_path=os.path.join(cmd_args.model_dir, "config.yaml"),
                use_fp16=cmd_args.fp16,
                use_deepspeed=cmd_args.deepspeed,
                use_cuda_kernel=cmd_args.cuda_kernel,
                )
# æ”¯æŒçš„è¯­è¨€åˆ—è¡¨
LANGUAGES = {
    "ä¸­æ–‡": "zh_CN",
    "English": "en_US"
}
EMO_CHOICES = [i18n("ä¸éŸ³è‰²å‚è€ƒéŸ³é¢‘ç›¸åŒ"),
                i18n("ä½¿ç”¨æƒ…æ„Ÿå‚è€ƒéŸ³é¢‘"),
                i18n("ä½¿ç”¨æƒ…æ„Ÿå‘é‡æ§åˆ¶"),
                i18n("ä½¿ç”¨æƒ…æ„Ÿæè¿°æ–‡æœ¬æ§åˆ¶")]
os.makedirs("outputs/tasks",exist_ok=True)
os.makedirs("prompts",exist_ok=True)

MAX_LENGTH_TO_USE_SPEED = 70
with open("/gemini/code/index-tts/examples/cases.jsonl", "r", encoding="utf-8") as f:
    example_cases = []
    for line in f:
        line = line.strip()
        if not line:
            continue
        example = json.loads(line)
        if example.get("emo_audio",None):
            emo_audio_path = os.path.join("examples",example["emo_audio"])
        else:
            emo_audio_path = None
        example_cases.append([os.path.join("examples", example.get("prompt_audio", "sample_prompt.wav")),
                              EMO_CHOICES[example.get("emo_mode",0)],
                              example.get("text"),
                             emo_audio_path,
                             example.get("emo_weight",1.0),
                             example.get("emo_text",""),
                             example.get("emo_vec_1",0),
                             example.get("emo_vec_2",0),
                             example.get("emo_vec_3",0),
                             example.get("emo_vec_4",0),
                             example.get("emo_vec_5",0),
                             example.get("emo_vec_6",0),
                             example.get("emo_vec_7",0),
                             example.get("emo_vec_8",0)]
                             )


def gen_single(emo_control_method,prompt, text,
               emo_ref_path, emo_weight,
               vec1, vec2, vec3, vec4, vec5, vec6, vec7, vec8,
               emo_text,emo_random,
               max_text_tokens_per_segment=120,
                *args, progress=gr.Progress()):
    output_path = None
    if not output_path:
        output_path = os.path.join("outputs", f"spk_{int(time.time())}.wav")
    # set gradio progress
    tts.gr_progress = progress
    do_sample, top_p, top_k, temperature, \
        length_penalty, num_beams, repetition_penalty, max_mel_tokens = args
    kwargs = {
        "do_sample": bool(do_sample),
        "top_p": float(top_p),
        "top_k": int(top_k) if int(top_k) > 0 else None,
        "temperature": float(temperature),
        "length_penalty": float(length_penalty),
        "num_beams": num_beams,
        "repetition_penalty": float(repetition_penalty),
        "max_mel_tokens": int(max_mel_tokens),
    }
    if type(emo_control_method) is not int:
        emo_control_method = emo_control_method.value
    if emo_control_method == 0:  # emotion from speaker
        emo_ref_path = None  # remove external reference audio
        emo_weight = 1.0
    if emo_control_method == 1:  # emotion from reference audio
        # emo_weight = emo_weight
        pass
    if emo_control_method == 2:  # emotion from custom vectors
        vec = [vec1, vec2, vec3, vec4, vec5, vec6, vec7, vec8]
        if sum(vec) > 1.5:
            gr.Warning(i18n("æƒ…æ„Ÿå‘é‡ä¹‹å’Œä¸èƒ½è¶…è¿‡1.5ï¼Œè¯·è°ƒæ•´åé‡è¯•ã€‚"))
            return
    else:
        # don't use the emotion vector inputs for the other modes
        vec = None

    if emo_text == "":
        # erase empty emotion descriptions; `infer()` will then automatically use the main prompt
        emo_text = None

    print(f"Emo control mode:{emo_control_method},weight:{emo_weight},vec:{vec}")
    output = tts.infer(spk_audio_prompt=prompt, text=text,
                        output_path=output_path,
                        emo_audio_prompt=emo_ref_path, emo_alpha=emo_weight,
                        emo_vector=vec,
                        use_emo_text=(emo_control_method==3), emo_text=emo_text,use_random=emo_random,
                        verbose=cmd_args.verbose,
                        max_text_tokens_per_segment=int(max_text_tokens_per_segment),
                        **kwargs)
    
    # ===== ä¿®æ”¹ï¼šä½¿ç”¨ WebSocket ä¸Šä¼  =====
    print(f"ğŸµ Inference complete, uploading result...")
    s3_url = upload_to_relay_server(output)
    if s3_url:
        print(f"âœ… File uploaded to S3: {s3_url}")
    else:
        print(f"âš ï¸ S3 upload failed, file saved locally at: {output}")
    # ===== ä¿®æ”¹ç»“æŸ =====

    return gr.update(value=output, visible=True)

def update_prompt_audio():
    update_button = gr.update(interactive=True)
    return update_button

with gr.Blocks(title="IndexTTS Demo") as demo:
    mutex = threading.Lock()
    gr.HTML('''
    <h2><center>IndexTTS2: A Breakthrough in Emotionally Expressive and Duration-Controlled Auto-Regressive Zero-Shot Text-to-Speech</h2>
<p align="center">
<a href='https://arxiv.org/abs/2506.21619'><img src='https://img.shields.io/badge/ArXiv-2506.21619-red'></a>
</p>
    ''')

    with gr.Tab(i18n("éŸ³é¢‘ç”Ÿæˆ")):
        with gr.Row():
            os.makedirs("prompts",exist_ok=True)
            prompt_audio = gr.Audio(label=i18n("éŸ³è‰²å‚è€ƒéŸ³é¢‘"),key="prompt_audio",
                                    sources=["upload","microphone"],type="filepath")
            prompt_list = os.listdir("prompts")
            default = ''
            if prompt_list:
                default = prompt_list[0]
            with gr.Column():
                input_text_single = gr.TextArea(label=i18n("æ–‡æœ¬"),key="input_text_single", placeholder=i18n("è¯·è¾“å…¥ç›®æ ‡æ–‡æœ¬"), info=f"{i18n('å½“å‰æ¨¡å‹ç‰ˆæœ¬')}{tts.model_version or '1.0'}")
                gen_button = gr.Button(i18n("ç”Ÿæˆè¯­éŸ³"), key="gen_button",interactive=True)
            output_audio = gr.Audio(label=i18n("ç”Ÿæˆç»“æœ"), visible=True,key="output_audio")
        with gr.Accordion(i18n("åŠŸèƒ½è®¾ç½®")):
            # æƒ…æ„Ÿæ§åˆ¶é€‰é¡¹éƒ¨åˆ†
            with gr.Row():
                emo_control_method = gr.Radio(
                    choices=EMO_CHOICES,
                    type="index",
                    value=EMO_CHOICES[0],label=i18n("æƒ…æ„Ÿæ§åˆ¶æ–¹å¼"))
        # æƒ…æ„Ÿå‚è€ƒéŸ³é¢‘éƒ¨åˆ†
        with gr.Group(visible=False) as emotion_reference_group:
            with gr.Row():
                emo_upload = gr.Audio(label=i18n("ä¸Šä¼ æƒ…æ„Ÿå‚è€ƒéŸ³é¢‘"), type="filepath")

        # æƒ…æ„Ÿéšæœºé‡‡æ ·
        with gr.Row(visible=False) as emotion_randomize_group:
            emo_random = gr.Checkbox(label=i18n("æƒ…æ„Ÿéšæœºé‡‡æ ·"), value=False)

        # æƒ…æ„Ÿå‘é‡æ§åˆ¶éƒ¨åˆ†
        with gr.Group(visible=False) as emotion_vector_group:
            with gr.Row():
                with gr.Column():
                    vec1 = gr.Slider(label=i18n("å–œ"), minimum=0.0, maximum=1.4, value=0.0, step=0.05)
                    vec2 = gr.Slider(label=i18n("æ€’"), minimum=0.0, maximum=1.4, value=0.0, step=0.05)
                    vec3 = gr.Slider(label=i18n("å“€"), minimum=0.0, maximum=1.4, value=0.0, step=0.05)
                    vec4 = gr.Slider(label=i18n("æƒ§"), minimum=0.0, maximum=1.4, value=0.0, step=0.05)
                with gr.Column():
                    vec5 = gr.Slider(label=i18n("åŒæ¶"), minimum=0.0, maximum=1.4, value=0.0, step=0.05)
                    vec6 = gr.Slider(label=i18n("ä½è½"), minimum=0.0, maximum=1.4, value=0.0, step=0.05)
                    vec7 = gr.Slider(label=i18n("æƒŠå–œ"), minimum=0.0, maximum=1.4, value=0.0, step=0.05)
                    vec8 = gr.Slider(label=i18n("å¹³é™"), minimum=0.0, maximum=1.4, value=0.0, step=0.05)

        with gr.Group(visible=False) as emo_text_group:
            with gr.Row():
                emo_text = gr.Textbox(label=i18n("æƒ…æ„Ÿæè¿°æ–‡æœ¬"), placeholder=i18n("è¯·è¾“å…¥æƒ…ç»ªæè¿°ï¼ˆæˆ–ç•™ç©ºä»¥è‡ªåŠ¨ä½¿ç”¨ç›®æ ‡æ–‡æœ¬ä½œä¸ºæƒ…ç»ªæè¿°ï¼‰"), value="", info=i18n("ä¾‹å¦‚ï¼šé«˜å…´ï¼Œæ„¤æ€’ï¼Œæ‚²ä¼¤ç­‰"))

        with gr.Row(visible=False) as emo_weight_group:
            emo_weight = gr.Slider(label=i18n("æƒ…æ„Ÿæƒé‡"), minimum=0.0, maximum=1.6, value=0.8, step=0.01)

        with gr.Accordion(i18n("é«˜çº§ç”Ÿæˆå‚æ•°è®¾ç½®"), open=False):
            with gr.Row():
                with gr.Column(scale=1):
                    gr.Markdown(f"**{i18n('GPT2 é‡‡æ ·è®¾ç½®')}** _{i18n('å‚æ•°ä¼šå½±å“éŸ³é¢‘å¤šæ ·æ€§å’Œç”Ÿæˆé€Ÿåº¦è¯¦è§')} [Generation strategies](https://huggingface.co/docs/transformers/main/en/generation_strategies)._")
                    with gr.Row():
                        do_sample = gr.Checkbox(label="do_sample", value=True, info=i18n("æ˜¯å¦è¿›è¡Œé‡‡æ ·"))
                        temperature = gr.Slider(label="temperature", minimum=0.1, maximum=2.0, value=0.8, step=0.1)
                    with gr.Row():
                        top_p = gr.Slider(label="top_p", minimum=0.0, maximum=1.0, value=0.8, step=0.01)
                        top_k = gr.Slider(label="top_k", minimum=0, maximum=100, value=30, step=1)
                        num_beams = gr.Slider(label="num_beams", value=3, minimum=1, maximum=10, step=1)
                    with gr.Row():
                        repetition_penalty = gr.Number(label="repetition_penalty", precision=None, value=10.0, minimum=0.1, maximum=20.0, step=0.1)
                        length_penalty = gr.Number(label="length_penalty", precision=None, value=0.0, minimum=-2.0, maximum=2.0, step=0.1)
                    max_mel_tokens = gr.Slider(label="max_mel_tokens", value=1500, minimum=50, maximum=tts.cfg.gpt.max_mel_tokens, step=10, info=i18n("ç”ŸæˆTokenæœ€å¤§æ•°é‡ï¼Œè¿‡å°å¯¼è‡´éŸ³é¢‘è¢«æˆªæ–­"), key="max_mel_tokens")
                with gr.Column(scale=2):
                    gr.Markdown(f'**{i18n("åˆ†å¥è®¾ç½®")}** _{i18n("å‚æ•°ä¼šå½±å“éŸ³é¢‘è´¨é‡å’Œç”Ÿæˆé€Ÿåº¦")}_')
                    with gr.Row():
                        initial_value = max(20, min(tts.cfg.gpt.max_text_tokens, cmd_args.gui_seg_tokens))
                        max_text_tokens_per_segment = gr.Slider(
                            label=i18n("åˆ†å¥æœ€å¤§Tokenæ•°"), value=initial_value, minimum=20, maximum=tts.cfg.gpt.max_text_tokens, step=2, key="max_text_tokens_per_segment",
                            info=i18n("å»ºè®®80~200ä¹‹é—´ï¼Œå€¼è¶Šå¤§ï¼Œåˆ†å¥è¶Šé•¿ï¼›å€¼è¶Šå°ï¼Œåˆ†å¥è¶Šç¢ï¼›è¿‡å°è¿‡å¤§éƒ½å¯èƒ½å¯¼è‡´éŸ³é¢‘è´¨é‡ä¸é«˜"),
                        )
                    with gr.Accordion(i18n("é¢„è§ˆåˆ†å¥ç»“æœ"), open=True) as segments_settings:
                        segments_preview = gr.Dataframe(
                            headers=[i18n("åºå·"), i18n("åˆ†å¥å†…å®¹"), i18n("Tokenæ•°")],
                            key="segments_preview",
                            wrap=True,
                        )
            advanced_params = [
                do_sample, top_p, top_k, temperature,
                length_penalty, num_beams, repetition_penalty, max_mel_tokens,
            ]
        
        if len(example_cases) > 0:
            gr.Examples(
                examples=example_cases,
                examples_per_page=20,
                inputs=[prompt_audio,
                        emo_control_method,
                        input_text_single,
                        emo_upload,
                        emo_weight,
                        emo_text,
                        vec1,vec2,vec3,vec4,vec5,vec6,vec7,vec8]
            )

    def on_input_text_change(text, max_text_tokens_per_segment):
        if text and len(text) > 0:
            text_tokens_list = tts.tokenizer.tokenize(text)

            segments = tts.tokenizer.split_segments(text_tokens_list, max_text_tokens_per_segment=int(max_text_tokens_per_segment))
            data = []
            for i, s in enumerate(segments):
                segment_str = ''.join(s)
                tokens_count = len(s)
                data.append([i, segment_str, tokens_count])
            return {
                segments_preview: gr.update(value=data, visible=True, type="array"),
            }
        else:
            df = pd.DataFrame([], columns=[i18n("åºå·"), i18n("åˆ†å¥å†…å®¹"), i18n("Tokenæ•°")])
            return {
                segments_preview: gr.update(value=df),
            }
    def on_method_select(emo_control_method):
        if emo_control_method == 1:  # emotion reference audio
            return (gr.update(visible=True),
                    gr.update(visible=False),
                    gr.update(visible=False),
                    gr.update(visible=False),
                    gr.update(visible=True)
                    )
        elif emo_control_method == 2:  # emotion vectors
            return (gr.update(visible=False),
                    gr.update(visible=True),
                    gr.update(visible=True),
                    gr.update(visible=False),
                    gr.update(visible=True)
                    )
        elif emo_control_method == 3:  # emotion text description
            return (gr.update(visible=False),
                    gr.update(visible=True),
                    gr.update(visible=False),
                    gr.update(visible=True),
                    gr.update(visible=True)
                    )
        else:  # 0: same as speaker voice
            return (gr.update(visible=False),
                    gr.update(visible=False),
                    gr.update(visible=False),
                    gr.update(visible=False),
                    gr.update(visible=False)
                    )

    emo_control_method.select(on_method_select,
        inputs=[emo_control_method],
        outputs=[emotion_reference_group,
                 emotion_randomize_group,
                 emotion_vector_group,
                 emo_text_group,
                 emo_weight_group]
    )

    input_text_single.change(
        on_input_text_change,
        inputs=[input_text_single, max_text_tokens_per_segment],
        outputs=[segments_preview]
    )
    max_text_tokens_per_segment.change(
        on_input_text_change,
        inputs=[input_text_single, max_text_tokens_per_segment],
        outputs=[segments_preview]
    )
    prompt_audio.upload(update_prompt_audio,
                         inputs=[],
                         outputs=[gen_button])

    gen_button.click(gen_single,
                     inputs=[emo_control_method,prompt_audio, input_text_single, emo_upload, emo_weight,
                             vec1, vec2, vec3, vec4, vec5, vec6, vec7, vec8,
                             emo_text,emo_random,
                             max_text_tokens_per_segment,
                             *advanced_params,
                     ],
                     outputs=[output_audio],
                     api_name="generate")



if __name__ == "__main__":
    demo.queue(20)
    demo.launch(server_name=cmd_args.host, server_port=cmd_args.port)
